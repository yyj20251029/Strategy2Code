{"paper_id": "ADDPG", "title": "Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement Learning for Stock Portfolio Allocation", "short_name": "ADDPG", "keywords": ["adaptive DDPG", "deep reinforcement learning", "portfolio allocation", "stock trading", "optimistic learning", "pessimistic learning", "Rescorla-Wagner", "time series", "Dow Jones 30"], "abstract": [{"text": "The paper proposes an Adaptive Deep Deterministic Policy Gradient (Adaptive DDPG) scheme for stock portfolio allocation. The method incorporates optimistic and pessimistic reinforcement learning via an asymmetric Rescorla\u2013Wagner style update (RW\u00b1) that responds differently to positive and negative prediction errors. Using daily prices of Dow Jones 30 component stocks, the authors train an Adaptive DDPG agent and compare it against vanilla DDPG, the Dow Jones Industrial Average (DJIA) index, and traditional mean-variance and minimum-variance portfolio allocation strategies. Empirical results show that Adaptive DDPG achieves higher final portfolio value, annualized return, and Sharpe ratio than all baselines."}], "sections": [{"heading": "1 INTRODUCTION", "text": "The introduction reviews modern portfolio theory (MPT) and its traditional mean\u2013variance framework, where investors select portfolios along an efficient frontier that trades off expected return and volatility. While MPT is foundational, the authors highlight several criticisms: investors are not always rational, markets are not perfectly efficient, and asset returns are not independent during crises. These limitations motivate data-driven approaches that can adapt to complex, dynamic market conditions. The paper proposes to model stock trading as a Markov Decision Process (MDP) and to use deep reinforcement learning, specifically Deep Deterministic Policy Gradient (DDPG), to obtain portfolio allocation strategies. To account for behavioral-finance style optimism and pessimism about market regimes (bull versus bear), they further introduce an adaptive reinforcement learning mechanism that updates more aggressively after positive or negative surprises."}, {"heading": "2 PORTFOLIO ALLOCATION AND MARKET ENVIRONMENT", "text": "This section briefly revisits classical portfolio allocation. In the mean\u2013variance framework, a portfolio of m assets has weights w = (w1, \u2026, wm) and annualized expected return \u00b5_p = w^T \u00b5, while the variance \u03c3_p^2 = w^T \u03a3_p w depends on the covariance matrix \u03a3_p of asset returns. Optimization problems include maximizing the Sharpe ratio under box constraints wi \u2208 [0, 0.2] and sum-to-one, or minimizing variance subject to full investment. The authors then discuss limitations of MPT assumptions: rational investors, efficient markets, and independent investments often fail in practice, especially around bubbles and crashes. They emphasize that macroeconomic factors and market micro-structure (bullish or bearish sentiment, herd behavior, short selling) significantly affect stock prices. Using Dow Jones 30 data, the paper compares efficient frontiers before and after the 2008\u20132009 financial crisis, showing lower returns and higher volatility during the crisis and improved risk\u2013return trade-offs afterward."}, {"heading": "3 MDP FORMULATION AND BASIC RL CONCEPTS", "text": "The stock trading problem is modeled as a finite Markov Decision Process with state space S, action space A, transition probabilities P^a_{ss'} = Pr{s_{t+1} = s' | s_t = s, a_t = a}, and expected rewards R^a_{ss'} = E{r_{t+1} | s_t = s, a_t = a, s_{t+1} = s'}. Standard reinforcement learning quantities are introduced: discounted return R_t = \u2211_{i=t}^T \u03b3^{i-t} r(s_i, a_i, s_{i+1}), policy \u03c0(s) mapping states to actions, and action-value function Q^\u03c0(s, a) = E[R_t | s_t = s, a_t = a, \u03c0]. The authors argue that dynamic programming is infeasible for portfolio problems due to large continuous state and action spaces, making function-approximation-based deep RL a more practical choice."}, {"heading": "3.1 STOCK TRADING AS AN MDP", "text": "For portfolio allocation, the environment state at time t is defined as s_t = [p_t, w_t, b_t], where p_t \u2208 R_+^D contains current stock prices for D assets, w_t = (w_{1,t}, \u2026, w_{D,t})^T are portfolio weights with w_i \u2208 [0,1] and \u2211_{i=1}^D w_i = 1, and b_t is the remaining cash balance. Actions a_t are continuous adjustments to the holdings of each stock; conceptually, each asset can be bought, sold, or held by changing its weight. The portfolio value equals the sum of stock equity p_t^T w_t plus cash b_t. After an action is executed and prices move to p_{t+1}, the new state s_{t+1} and one-step reward r(s_t, a_t, s_{t+1}) are determined by the change in portfolio value. The objective is to learn a policy that maximizes expected cumulative discounted rewards and, equivalently, maximizes the value of the portfolio at a future horizon t_f."}, {"heading": "3.2 MODIFIED RESCORLA\u2013WAGNER MODEL (RW) AND RW\u00b1", "text": "To model learning from market feedback, the authors adopt a Rescorla\u2013Wagner style Q-learning update. For each state\u2013action pair (s_t, a_t), a scalar value Q^\u03c0(s_t, a_t) represents the expected reward of taking that action. The baseline update is Q_{t+1} = Q_t + \u03b1 \u03b4(t), where \u03b1 is a learning rate and \u03b4(t) = r(s_t, a_t, s_{t+1}) \u2212 Q_t is the prediction error between realized reward and prior expectation. In the modified RW\u00b1 model, separate learning rates \u03b1_+ and \u03b1_- are used for positive and negative prediction errors: if \u03b4(t) > 0, Q is updated with \u03b1_+ \u03b4(t); if \u03b4(t) < 0, with \u03b1_- \u03b4(t). This allows the agent to learn differently from good and bad outcomes, capturing optimistic or pessimistic behavior. A softmax decision rule converts Q-values for discrete options (buy, hold, sell) into action-selection probabilities, with an inverse temperature parameter \u03b2 controlling the exploration\u2013exploitation trade-off."}, {"heading": "3.3 ADAPTIVE DDPG ALGORITHM", "text": "The core of the method is an adaptive Deep Deterministic Policy Gradient (Adaptive DDPG) algorithm that blends continuous-action DDPG with the RW\u00b1 update. A standard DDPG agent employs an actor network \u03bc(s | \u03b8^\u03bc) producing deterministic actions and a critic network Q(s, a | \u03b8^Q) estimating Q-values. Target networks \u03bc' and Q' and an experience replay buffer R stabilize learning. The critic is trained to minimize the mean-squared error between Q(s_t, a_t) and target y_t = r_t + \u03b3 Q'(s_{t+1}, \u03bc'(s_{t+1} | \u03b8^{\u03bc'}, \u03b8^{Q'})). In Adaptive DDPG, the prediction error \u03b4(t) from the RW\u00b1 formulation modulates how Q(s, a) and the critic parameters are updated: the learning rate adjustment depends on whether \u03b4(t) is positive or negative, representing optimistic or pessimistic learning under bull or bear market conditions. Two random processes N^+ and N^- are used to inject exploration noise into the actor output under positive and negative environments. After each mini-batch update, the target networks are softly updated via \u03b8^{Q'} \u2190 \u03c4 \u03b8^Q + (1 \u2212 \u03c4) \u03b8^{Q'} and \u03b8^{\u03bc'} \u2190 \u03c4 \u03b8^\u03bc + (1 \u2212 \u03c4) \u03b8^{\u03bc'}, where \u03c4 is a small smoothing constant."}, {"heading": "4 EXPERIMENTS AND DATA", "text": "The experimental setup uses daily prices of the 30 component stocks of the Dow Jones Industrial Average as the stock pool. Data span from January 1, 2001 to September 30, 2018 and are sourced from the Compustat database via WRDS. The period from 2001\u20132013 (3,268 trading days) serves as the training set; the period from January 2, 2014 to October 2, 2018 (1,190 trading days) is used for testing. The Adaptive DDPG agent is trained on the training set and then evaluated on the held-out test set. Learning rates for RW\u00b1 are chosen such that \u03b1_+ = 1 and \u03b1_- = 0 during the test period, and the two noise processes N^+ and N^- are defined so that N^+ is a standard normal process while N^- generates only negative values, reflecting asymmetric exploration under different market conditions. Baselines include vanilla DDPG without RW\u00b1, the DJIA index as a buy-and-hold benchmark, and traditional mean-variance and minimum-variance portfolios derived from Markowitz optimization."}, {"heading": "4.2 PERFORMANCE RESULTS", "text": "Performance is evaluated using final portfolio value, annualized return, annualized standard deviation of returns, and Sharpe ratio. Starting from an initial portfolio value of 10,000, the Adaptive DDPG strategy achieves a final value of about 21,880, compared to 18,156 for vanilla DDPG, 16,089 for DJIA, 16,333 for the minimum-variance portfolio, and 19,632 for the mean-variance portfolio. Adaptive DDPG obtains an annualized return of 18.84% with volatility 11.59% and a Sharpe ratio of 1.63, outperforming DDPG (14.71%, Sharpe 1.01), DJIA (11.36%, Sharpe 0.91), and traditional portfolios. Equity curves plotted over the 2014\u20132018 test period show that Adaptive DDPG stays above all baselines for most of the time horizon. These results indicate that incorporating optimistic and pessimistic reinforcement learning into DDPG yields superior portfolio allocation strategies under varying market environments."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "The paper concludes that Adaptive DDPG, which integrates asymmetric RW\u00b1 updates into the DDPG framework, can effectively improve stock portfolio allocation performance relative to vanilla DDPG, market benchmarks, and classical mean-variance methods. By allowing different learning behavior for positive and negative prediction errors, the model can better adapt to bull and bear market regimes and extract more value from noisy financial time series. The authors suggest extending the work by applying the method to larger-scale datasets, combining it with anomaly detection and price-prediction components, and incorporating text-based signals from news and social media into the state representation to further enhance robustness and predictive power."}]}